{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, make sure that this jupyter notebook was launched using the 'pySpark' command and not the 'jupyter notebook' command.\n",
    "\n",
    "This notebook is for Python 2.7 - The only difference for python 3.X is that the print statements need () \n",
    "\n",
    "The 'data' folder and this notebook must have same parent directory.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Pyspark with Jupyter on MacBook:\n",
    "Here is a fail proof way to install pySpark on Mac\n",
    "```\n",
    "brew install jupyter\n",
    "brew install scala\n",
    "brew install apache-spark\n",
    "brew cask install caskroom/versions/java8\n",
    "```\n",
    "Add to your bash_profile the environment vars. Run following commands:\n",
    "```\n",
    "sudo nano ~./bash_profile\n",
    "```\n",
    "Add to file:\n",
    "\n",
    "```\n",
    "if which pyspark > /dev/null; then\n",
    "  export SPARK_HOME=\"/usr/local/Cellar/apache-spark/2.3.1/libexec/\"\n",
    "  export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/build:$PYTHONPATH\n",
    "  export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\n",
    "fi\n",
    "\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "```\n",
    "\n",
    "cntrl+X key lets you exit.\n",
    "Select Y for saving\n",
    "\n",
    "Compile the script by\n",
    "```\n",
    "source ~/.bash_profile\n",
    "```\n",
    "And thats all!\n",
    "Run pySpark by typing\n",
    "\n",
    "```\n",
    "pySpark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is Apache Spark?</b>\n",
    "\n",
    "A 'distributed' computing engine. \n",
    "<br>Analogy: A team working on a big project.\n",
    "\n",
    "Apache spark has 'master node' and 'slave nodes' which together form the cluster\n",
    "<br>Analogy: A team has a leader and multiple members\n",
    "\n",
    "The Spark 'Job' is submitted to the Cluster's master\n",
    "<br>A project is assigned to a team leader\n",
    "\n",
    "The master then distributes (assigns) the work between the slaves and awaits the response(result)\n",
    "<br>\n",
    "Its a non-agile team :P\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Diagrams we often see\n",
    "<img src=\"assetsPySparkTut/2.jpg\">\n",
    "<img style=\"height:400px\" src=\"assetsPySparkTut/7.png\">\n",
    "<img style=\"height:400px\" src=\"assetsPySparkTut/8.jpg\">\n",
    "<img src=\"assetsPySparkTut/1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What actually happens?</b>\n",
    "\n",
    "1. The first layer is the interpreter, Spark uses a Scala interpreter, with some modifications.\n",
    "\n",
    "2. As you enter your code in spark console (creating RDD’s and applying operators), Spark creates a operator graph.\n",
    "\n",
    "3. When the user runs an action (like collect), the Graph is submitted to a DAG Scheduler. The DAG scheduler divides operator graph into (map and reduce) stages.\n",
    "\n",
    "4. A stage is comprised of tasks based on partitions of the input data. The DAG scheduler pipelines operators together to optimize the graph. For e.g. Many map operators can be scheduled in a single stage. This optimization is key to Sparks performance. The final result of a DAG scheduler is a set of stages.\n",
    "\n",
    "5. The stages are passed on to the Task Scheduler. The task scheduler launches tasks via cluster manager. (Spark Standalone/Yarn/Mesos). The task scheduler doesn’t know about dependencies among stages.\n",
    "\n",
    "6. The Worker executes the tasks. A new JVM is started per job. The worker knows only about the code that is passed to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "<b>What happens (Simpler version):</b>\n",
    "\n",
    "A. Coding interface:\n",
    "    1. We write code (RDD, Dataframes, etc)\n",
    "B. Code interpretation (Spark Engine):\n",
    "    1. Code is converted into 'DAG - Directed Acyclic graphs' \n",
    "    2. DAG converted to tasks (Highly optimized) -> Imagine this as the 'project execution plan' for the team analogy\n",
    "C. Scheduling and task distribution\n",
    "    1. Uses DAG generated task schema to distribute the work load\n",
    "D. Task execution\n",
    "    1. Slaves execute the tasks and report back to master\n",
    "E. Results compilation\n",
    "    1. Master stiches the results together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Where does all of this happen?</b>\n",
    "\n",
    "<b>A.</b> Coding interface\n",
    "<img style=\"height:200px;\" src=\"assetsPySparkTut/3.jpg\">\n",
    "<img style=\"height:200px;\" src=\"assetsPySparkTut/4.jpg\">\n",
    "\n",
    "We use Zeppelin in our production environment.\n",
    " Key differences:<br>\n",
    "1. Every Notebook cell starts with declaration of 'Intrepretor type' -> %python, %pyspark, %sql\n",
    "2. You can use different intrepretors for different cells\n",
    "3. SQL intrepretor has inbuilt visualization tool\n",
    "4. Use Spark (%pyspark), save to 'Hive' (Distributed SQL) and then use %sql to load and visualize data!\n",
    "\n",
    "\n",
    "<img style=\"height:500px;\" src=\"assetsPySparkTut/6.png\">\n",
    "\n",
    "<b>B.</b> Code interpretation (Spark Engine),\n",
    "<b>C.</b> Scheduling and task distribution\n",
    "Both happen on the master node of Spark\n",
    "\n",
    "<b>D.</b> Task execution\n",
    "On individual slaves (They operate independently, unaware of each other)\n",
    "<b>E.</b> Results compilation\n",
    "    1. Master stiches the results together and reports them to users\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why is this powerful?</b>\n",
    "\n",
    "Consider example of 'JOIN' query after series of selects\n",
    "\n",
    "Explained on white board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Infrastructure we use</b>\n",
    "\n",
    "Explained on whiteboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Memory management - Where is data stored after every step?</b><br>\n",
    "Explained with diagram on white board  (RAM+Disk)\n",
    "\n",
    "<img style=\"height:400px;\" src=\"assetsPySparkTut/5.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data structures in Spark</b>\n",
    "'RDD' Resilient Distributed Dataset is the building block<br>\n",
    "'Dataframe' Similar to Numpy dataframes<br>\n",
    "'Dataset' similar to python dictionaries<br>\n",
    "<br>\n",
    "We care ONLY about Dataframes. Dataframes (dfs) are like SQL tables. We can run queries on these dfs just like SQL.<br>\n",
    "<b>Treat Dataframe+Spark+Python as a fancy new SQL tool</b>\n",
    "<br>\n",
    "Ignore all the complex backend, distributed stuff and scalability. Data scientists/Analysts should worry about data and not the system/infrastructure.\n",
    "\n",
    "If you are good at Python and SQL, you can easily become Spark expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lets start coding!</b><br>\n",
    "Things to remember -><br> \n",
    "1. Dataframe is like SQL table\n",
    "2. Spark can connect with ANY datasource (input/output)\n",
    "3. Remember the power available is tremendous, but optimize where ever possible (Its costly!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is Scala based system. All python code is converted to Scala and then run on the cluster.<br>\n",
    "Using notebooks hides many elements of Spark code from us. <br>\n",
    "Spark context (constructor for Spark) is variable 'sc'\n",
    "Spark SQL context (constructor for Spark SQL engine) is varialble 'sqlContext'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lets look at the Data architecture</b><br>\n",
    "Heirarchy in data:\n",
    "Growers ->\n",
    "Farms ->\n",
    "Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "#sqlContext.read.{fileType}({fileLocation})\n",
    "#Creates Dataframe out of our data!\n",
    "\n",
    "fields = sqlContext.read.json(\"dummyData/landing/field/*.json\")\n",
    "#fields = sqlContext.read.json(\"s3n://data-lake-us-east-2-549323063936-validated/NonExploded_New_data_field.json/*.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read growers and farms here\n",
    "\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing Schemas of data\n",
    "fields.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this schema mean?\n",
    "# |    field      |\n",
    "# | ______________|\n",
    "# |associations|..|\n",
    "# |            |  |\n",
    "# |            |  |\n",
    "# |            |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Print the schemas for grower and farms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT queries\n",
    "\n",
    "fields.select('field')\n",
    "#Returns a new dataframe. fields df remains unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print fields.field\n",
    "#Returns a Column dataStructure! Avoid converting to Columns as columns have limited use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fields.select('field').show()\n",
    "#Prints the first 20 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.select('field.*').show()\n",
    "#Notice the difference in what happened here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "fields.filter(col(\"field.deleted_at\").isNotNull())\n",
    "#Notice that it returns dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.filter(col(\"field.deleted_at\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.where(col(\"field.deleted_at\").isNotNull()).show()\n",
    "#Where clause works as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.where(\"fields.deleted_at != 'null'\").show()\n",
    "#Another way of using Where clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields['field']\n",
    "#Returns Col object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.filter(fields['field.deleted_at'].isNotNull()).show()\n",
    "#Another way of doing it right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Filter Fields that are NOT deleted. And then Remove the nested 'field' column. \n",
    "#Overwrite the dataframe fields with the new result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "#Apply the same transformation for Growers and farms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At this point, farms-fields-growers are all in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at renaming columns\n",
    "#Use withColumnRenamed(Old, New)\n",
    "\n",
    "fields.withColumnRenamed('id','grower_id_temp').show()\n",
    "\n",
    "fields.show()\n",
    "#Notice that fields remains unchanged as we dint overwrite the Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.select('associations').printSchema()\n",
    "#Select always returns Df. Converts nested columns to dfs using Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.select('associations.agrian.farm')\n",
    "#This wont work :(\n",
    "#There is a '.' in the name of the attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields.select('associations.`agrian.farm`').show()\n",
    "#Use this instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check type of columns\n",
    "for name, dtype in fields.dtypes:\n",
    "    print(name, dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dtype in fields.select('associations.`agrian.farm`').dtypes:\n",
    "    print(name, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploding Arrays in Df\n",
    "import pyspark.sql.functions as pys\n",
    "associations = fields.select('associations.*')\n",
    "associations.show()\n",
    "associations.withColumn(\"associationsExploded\", pys.explode(associations['`agrian.farm`'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joins in pyspark\n",
    "#df1.join(df2, df1.ColumnName == df2.columnName, typeOfJoin)\n",
    "\n",
    "#Suppose we have 2 dataframes with 'id1' for df1 and 'id2' for df2. \n",
    "#We want to left outer join df1 over df2 dfs over 1d1 and id2\n",
    "\n",
    "#df1.join(df2, df1.id1 == df2.id2, 'left_outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We often need to rename columns to avoid 2 columns with same names during joins\n",
    "fieldsToJoin = fields.withColumnRenamed('id', 'field_id').withColumnRenamed('associations', 'field_associations')\n",
    "fieldsToJoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "#Join all fields with farms\n",
    "#Use farm Id in associations of field ID as the join column\n",
    "\n",
    "#Discussion if farm will be joined with field or field over farm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframes as JSON\n",
    "farms.write.json('dummyData/validated1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining into one JSON\n",
    "farms.coalesce(1).write.json('dummyData/validated2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF User defined functions for transforming columns in Dfs\n",
    "#Suppose we need to make complex transformations on columns in DF\n",
    "#We use UDFs\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.sql.types import DoubleType, StringType, FloatType, IntegerType, BooleanType\n",
    "\n",
    "def isBoundaryPresent(columnElement):\n",
    "    if not columnElement:\n",
    "        return False\n",
    "    try:\n",
    "        columnElement = columnElement.strip()\n",
    "        columnElement = unicode(str(columnElement), \"utf-8\")\n",
    "        if columnElement == \"MULTIPOLYGON EMPTY\" or columnElement == None or columnElement == \"null\" or columnElement == \"\" or columnElement == \" \" or  columnElement == None:\n",
    "            return False\n",
    "            \n",
    "        if columnElement[:5] == \"MULTI\":\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    except:\n",
    "        print \"Error\"\n",
    "        return False\n",
    "    \n",
    "checkIfBoundaryPresent = udf(lambda z: isBoundaryPresent(z), BooleanType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNew = df.withColumn('isBoundaryPresent', checkIfBoundaryPresent(df.boundary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
